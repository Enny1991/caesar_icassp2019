{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from scipy.io import wavfile\n",
    "import re\n",
    "import pickle as pkl\n",
    "\n",
    "import tables\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_pos(pos):\n",
    "    if pos == 0:\n",
    "        pos = 1\n",
    "    if pos == 18:\n",
    "        pos = 17\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = '/Volumes/Elements/TIDIGITS/'\n",
    "mtr = 'CD1/TIDIGITS/TRAIN/MAN/'\n",
    "ftr = 'CD1/TIDIGITS/TRAIN/WOMAN/'\n",
    "mte = 'CD2/TIDIGITS/TEST/MAN/'\n",
    "fte = 'CD2/TIDIGITS/TEST/WOMAN/'\n",
    "\n",
    "old_separator = '/'\n",
    "new_separator = '\\\\'\n",
    "\n",
    "D = {'train': {'m': {}, 'f': {}}, 'test': {'m': {}, 'f': {}}}\n",
    "for path, task, gender in zip([mtr, ftr, mte, fte], ['train', 'train', 'test', 'test'], ['m', 'f', 'm', 'f']):\n",
    "    _list = [i for i in os.listdir(basedir + path)if len(i) == 2]\n",
    "    for spk in _list:\n",
    "        _wavs = [i for i in os.listdir(basedir + path + spk + '/') if \"WAV\" in i]\n",
    "        D[task][gender][spk] = [path + spk + '/' + i for i in _wavs]\n",
    "\n",
    "to_digit = {'O': 0,'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, 'Z': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 6000\n",
    "with open('train_set.txt', 'w') as wavs:\n",
    "    for i in range(n_samples):\n",
    "        rnd_gnd1 = np.random.choice(['m', 'f'])\n",
    "        rnd_spk1 = np.random.choice(D['train'][rnd_gnd1].keys())\n",
    "        rnd_wav1 = np.random.choice(D['train'][rnd_gnd1][rnd_spk1])\n",
    "        \n",
    "        rnd_gnd2 = np.random.choice(['m', 'f'])\n",
    "        rnd_spk2 = np.random.choice(D['train'][rnd_gnd2].keys())\n",
    "        rnd_wav2 = np.random.choice(D['train'][rnd_gnd2][rnd_spk2])\n",
    "        \n",
    "        ann1 = \"\".join([str(to_digit[i]) for i in rnd_wav1.split('/')[-1][:-5]])\n",
    "        ann2 = \"\".join([str(to_digit[i]) for i in rnd_wav2.split('/')[-1][:-5]])\n",
    "        nn = rnd_spk1 + ann1 + rnd_spk2 + ann2\n",
    "        wavs.write(rnd_wav1.replace(old_separator, new_separator) + ' ' + rnd_wav2.replace(old_separator, new_separator) + ' ' + ann1 + ' ' + ann2 + ' ' + nn +'\\n')\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "with open('test_set.txt', 'w') as wavs:\n",
    "    for i in range(n_samples):\n",
    "        rnd_gnd1 = np.random.choice(['m', 'f'])\n",
    "        rnd_spk1 = np.random.choice(D['test'][rnd_gnd1].keys())\n",
    "        rnd_wav1 = np.random.choice(D['test'][rnd_gnd1][rnd_spk1])\n",
    "        \n",
    "        rnd_gnd2 = np.random.choice(['m', 'f'])\n",
    "        rnd_spk2 = np.random.choice(D['test'][rnd_gnd2].keys())\n",
    "        rnd_wav2 = np.random.choice(D['test'][rnd_gnd2][rnd_spk2])\n",
    "        \n",
    "        ann1 = \"\".join([str(to_digit[i]) for i in rnd_wav1.split('/')[-1][:-5]])\n",
    "        ann2 = \"\".join([str(to_digit[i]) for i in rnd_wav2.split('/')[-1][:-5]])\n",
    "        \n",
    "        nn = rnd_spk1 + ann1 + rnd_spk2 + ann2\n",
    "        \n",
    "        wavs.write(rnd_wav1.replace(old_separator, new_separator) + ' ' + rnd_wav2.replace(old_separator, new_separator) + ' ' + ann1 + ' ' + ann2 + ' ' + nn + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_mix\n",
    "with open('train_set.txt', 'r') as wavs, open('description_log_mix.csv', 'w') as csv, open('test_set.txt', 'r') as wavs_t:\n",
    "    csv.write('mode,group,modality,key,channel,path,label\\n')\n",
    "    count = 0\n",
    "    for l in wavs.readlines():\n",
    "        _, utt1, utt2 = re.compile('[A-Z]+').split(l.split(' ')[-1])\n",
    "        \n",
    "        csv.write('default,train,audio,train_{},1,'.format(count))\n",
    "        csv.write('train/' + l.split(' ')[-1].strip() + '.wav,')\n",
    "        csv.write(utt1.replace('0', 'Z') + utt2.replace('0', 'Z'))\n",
    "        count += 1\n",
    "    \n",
    "    count = 0\n",
    "    for l in wavs_t.readlines():\n",
    "        _, utt1, utt2 = re.compile('[A-Z]+').split(l.split(' ')[-1])\n",
    "        \n",
    "        csv.write('default,test,audio,test_{},1,'.format(count))\n",
    "        csv.write('test/' + l.split(' ')[-1].strip() + '.wav,')\n",
    "        csv.write(utt1.replace('0', 'Z') + utt2.replace('0', 'Z'))\n",
    "        count += 1\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separated original fbank of only the registered ones\n",
    "with open('train_set.txt', 'r') as wavs, open('description_sep_log.csv', 'w') as csv, open('test_set.txt', 'r') as wavs_t:\n",
    "    csv.write('mode,group,modality,key,channel,path,label\\n')\n",
    "    count = 0\n",
    "    for l in wavs.readlines():\n",
    "        utt1, utt2 = l.split(' ')[2], l.split(' ')[3]\n",
    "        \n",
    "        csv.write('default,train,audio,train_{},1,'.format(count))\n",
    "        csv.write('train/' + l.split(' ')[0].strip().replace('\\\\', '/') + '.wav,')\n",
    "        csv.write(utt1.replace('0', 'Z') + '\\n')\n",
    "        count += 1\n",
    "        \n",
    "        csv.write('default,train,audio,train_{},1,'.format(count))\n",
    "        csv.write('train/' + l.split(' ')[1].strip().replace('\\\\', '/') + '.wav,')\n",
    "        csv.write(utt2.replace('0', 'Z') + '\\n')\n",
    "        count += 1\n",
    "    \n",
    "    count = 0\n",
    "    for l in wavs_t.readlines():\n",
    "        utt1, utt2 = l.split(' ')[2], l.split(' ')[3]\n",
    "        \n",
    "        csv.write('default,test,audio,test_{},1,'.format(count))\n",
    "        csv.write('test/' + l.split(' ')[0].strip().replace('\\\\', '/') + '.wav,')\n",
    "        csv.write(utt1.replace('0', 'Z') + '\\n')\n",
    "        count += 1\n",
    "        \n",
    "        csv.write('default,test,audio,test_{},1,'.format(count))\n",
    "        csv.write('test/' + l.split(' ')[1].strip().replace('\\\\', '/') + '.wav,')\n",
    "        csv.write(utt2.replace('0', 'Z') + '\\n')\n",
    "        count += 1\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixed spike fuatures\n",
    "with open('description_spk_mix.csv', 'w') as csv, open('train_set.txt', 'r') as wavs, open('test_set.txt', 'r') as wavs_t:\n",
    "    \n",
    "    csv.write('mode,group,modality,key,channel,path,label\\n')\n",
    "    count = 0\n",
    "    for l in wavs.readlines():\n",
    "        utt1, utt2 = l.split(' ')[2], l.split(' ')[3]\n",
    "        csv.write('default,train,feat,train_{},1,{},{}\\n'.format(count, count, utt1.replace('0', 'Z') + utt2.replace('0', 'Z')))\n",
    "        count += 1\n",
    "        \n",
    "    count = 0\n",
    "    for l in wavs_t.readlines():\n",
    "        utt1, utt2 = l.split(' ')[2], l.split(' ')[3]\n",
    "        csv.write('default,test,feat,test_{},1,{},{}\\n'.format(count, count, utt1.replace('0', 'Z') + utt2.replace('0', 'Z')))\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separated spike features\n",
    "basedir = '/Data/DATASETS/CAESAR_TIDIGITS/'\n",
    "data_train = pkl.load(open('all_sep_spk_train.pkl', 'r'))\n",
    "data_test = pkl.load(open('all_sep_spk_test.pkl', 'r'))\n",
    "ALL_ANGLES = {}\n",
    "\n",
    "with open('description_spk_sep.csv', 'w') as csv:\n",
    "    csv.write('mode,group,modality,key,channel,path,label\\n')\n",
    "    count = 0\n",
    "    for i, l in enumerate(data_train['lbl']):\n",
    "        csv.write('default,train,feat,train_{},1,{},{}\\n'.format(count, count, l.replace('0', 'Z')))\n",
    "        count += 1\n",
    "        \n",
    "    count = 0\n",
    "    for i, l in enumerate(data_test['lbl']):\n",
    "        csv.write('default,test,feat,test_{},1,{},{}\\n'.format(count, count, l.replace('0', 'Z')))\n",
    "        count += 1\n",
    "\n",
    "# filter for same position or adjacient position\n",
    "\n",
    "with open(basedir + 'log_train.txt') as t:\n",
    "    ALL_ANGLES['train'] = [x for x in t.readlines()]\n",
    "\n",
    "with open(basedir + 'log_test.txt') as t:\n",
    "    ALL_ANGLES['test'] = [x for x in t.readlines()]\n",
    "\n",
    "with open('description_spk_sep_noADJ.csv', 'w') as csv:\n",
    "    to_do_train = []\n",
    "    for l in ALL_ANGLES['train']:\n",
    "        pos1, pos2 = int(float(l.split(' ')[1]) // 10) , int(float(l.split(' ')[2]) // 10)\n",
    "        pos1, pos2 = correct_pos(pos1), correct_pos(pos2)\n",
    "        if np.abs(pos1 - pos2) > 1:\n",
    "            to_do_train.append(True)\n",
    "            to_do_train.append(True)\n",
    "        else:\n",
    "            to_do_train.append(False)\n",
    "            to_do_train.append(False)\n",
    "            \n",
    "    to_do_test = []\n",
    "    for l in ALL_ANGLES['test']:\n",
    "        pos1, pos2 = int(float(l.split(' ')[1]) // 10) , int(float(l.split(' ')[2]) // 10)\n",
    "        pos1, pos2 = correct_pos(pos1), correct_pos(pos2)\n",
    "        if np.abs(pos1 - pos2) > 1:\n",
    "            to_do_test.append(True)\n",
    "            to_do_test.append(True)\n",
    "        else:\n",
    "            to_do_test.append(False)\n",
    "            to_do_test.append(False)\n",
    "        \n",
    "    csv.write('mode,group,modality,key,channel,path,label\\n')\n",
    "    count = 0\n",
    "    for i, l in enumerate(data_train['lbl']):\n",
    "        if to_do_train[i]:\n",
    "            csv.write('default,train,feat,train_{},1,{},{}\\n'.format(count, count, l.replace('0', 'Z')))\n",
    "            count += 1\n",
    "\n",
    "    count = 0\n",
    "    for i, l in enumerate(data_test['lbl']):\n",
    "        if to_do_test[i]:\n",
    "            csv.write('default,test,feat,test_{},1,{},{}\\n'.format(count, count, l.replace('0', 'Z')))\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v07\n",
    "# separated spike features\n",
    "basedir = '/Data/DATASETS/CAESAR_TIDIGITS/'\n",
    "data_train = pkl.load(open('all_sep_spk_train_77__v07.pkl', 'r'))\n",
    "data_test = pkl.load(open('all_sep_spk_test_77__v07.pkl', 'r'))\n",
    "\n",
    "no_wake = ['0', 'Z', '1', '2', '3', '4', '5', '6', '8', '9']\n",
    "\n",
    "with open('description_spk_sep_77_v07.csv', 'w') as csv:\n",
    "        \n",
    "    csv.write('mode,group,modality,key,channel,path,label\\n')\n",
    "    count = 0\n",
    "    for i, l in enumerate(data_train['lbl']):\n",
    "        csv.write('default,train,feat,train_{},1,{},{}\\n'.format(count, count, replaceMultiple(l, no_wake, '1')))\n",
    "        count += 1\n",
    "\n",
    "    count = 0\n",
    "    for i, l in enumerate(data_test['lbl']):\n",
    "        csv.write('default,test,feat,test_{},1,{},{}\\n'.format(count, count, replaceMultiple(l, no_wake, '1')))\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000def000def000def\n"
     ]
    }
   ],
   "source": [
    "def replaceMultiple(mainString, toBeReplaces, newString):\n",
    "    # Iterate over the strings to be replaced\n",
    "    for elem in toBeReplaces :\n",
    "        # Check if string is in the main string\n",
    "        if elem in mainString :\n",
    "            # Replace the string\n",
    "            mainString = mainString.replace(elem, newString)\n",
    "    \n",
    "    return  mainString\n",
    "str1 = 'abcdefabcdefabcdef'\n",
    "print replaceMultiple(str1, ['0', 'Z','c'], '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projected spike features\n",
    "data_train = pkl.load(open('all_log_prj_train.pkl', 'r'))\n",
    "data_test = pkl.load(open('all_log_prj_test.pkl', 'r'))\n",
    "\n",
    "with open('description_log_prj.csv', 'w') as csv:\n",
    "    csv.write('mode,group,modality,key,channel,path,label\\n')\n",
    "    count = 0\n",
    "    for i, l in enumerate(data_train['lbl']):\n",
    "        csv.write('default,train,feat,train_{},1,{},{}\\n'.format(count, count, l.replace('0', 'Z')))\n",
    "        count += 1\n",
    "        \n",
    "    count = 0\n",
    "    for i, l in enumerate(data_test['lbl']):\n",
    "        csv.write('default,test,feat,test_{},1,{},{}\\n'.format(count, count, l.replace('0', 'Z')))\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables\n",
    "h5file = '/Data/Dropbox/tidigits_kaldi_fbank.h5'\n",
    "\n",
    "h5 = tables.open_file(h5file, 'r')\n",
    "dataset = h5.get_node(os.path.join(os.sep, 'default', 'train'))\n",
    "\n",
    "channels = h5.list_nodes(os.path.join(os.sep, 'default', 'train'))\n",
    "\n",
    "print len(channels[0].features)\n",
    "print channels[0].label[0]\n",
    "print channels[0].label_length[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables\n",
    "import os\n",
    "h5file = '/Data/DATASETS/CAESAR_TIDIGITS/tidigits_spk_sep.h5'\n",
    "\n",
    "h5 = tables.open_file(h5file, 'r')\n",
    "dataset = h5.get_node(os.path.join(os.sep, 'default', 'train'))\n",
    "\n",
    "channels = h5.list_nodes(os.path.join(os.sep, 'default', 'train'))\n",
    "\n",
    "print len(channels[0].features)\n",
    "print channels[0].label[1]\n",
    "print channels[0].label_length[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "subset = 'test'\n",
    "h5 = tables.open_file('/Data/DATASETS/CAESAR_TIDIGITS/tidigits_log_sep.h5', 'r')\n",
    "log_sep = h5.list_nodes(os.path.join(os.sep, 'default', subset))\n",
    "h5 = tables.open_file('/Data/DATASETS/CAESAR_TIDIGITS/tidigits_log_mix.h5', 'r')\n",
    "log_mix = h5.list_nodes(os.path.join(os.sep, 'default', subset))\n",
    "h5 = tables.open_file('/Data/DATASETS/CAESAR_TIDIGITS/tidigits_spk_sep_v02.h5', 'r')\n",
    "spk_sep = h5.list_nodes(os.path.join(os.sep, 'default', subset))\n",
    "h5 = tables.open_file('/Data/DATASETS/CAESAR_TIDIGITS/tidigits_spk_mix.h5', 'r')\n",
    "spk_mix = h5.list_nodes(os.path.join(os.sep, 'default', subset))\n",
    "h5 = tables.open_file('/Data/DATASETS/CAESAR_TIDIGITS/tidigits_log_prj_v02.h5', 'r')\n",
    "log_prj = h5.list_nodes(os.path.join(os.sep, 'default', subset))\n",
    "\n",
    "# print len(log_sep[0].features)\n",
    "# print len(log_mix[0].features)\n",
    "# print len(spk_sep[0].features)\n",
    "# print len(spk_mix[0].features)\n",
    "# print len(log_prj[0].features)\n",
    "\n",
    "IDX = 87\n",
    "# print \"===\"\n",
    "# print log_sep[0].label[IDX * 2]\n",
    "# print log_sep[0].label[IDX * 2 + 1]\n",
    "# print log_mix[0].label[IDX]\n",
    "# print spk_sep[0].label[IDX * 2]\n",
    "# print spk_sep[0].label[IDX * 2 + 1]\n",
    "print spk_mix[0].label[IDX]\n",
    "# print log_prj[0].label[IDX * 2]\n",
    "# print log_prj[0].label[IDX * 2 + 1]\n",
    "\n",
    "f, ax = plt.subplots(3, 3, figsize=(20,10))\n",
    "a = ax[0][0].imshow(np.reshape(log_mix[0].features[IDX], log_mix[0].feature_shape[IDX])[:, :41].T, aspect='auto')\n",
    "ax[0][0].set_title(log_mix[0].label[IDX])\n",
    "plt.colorbar(a, ax=ax[0][0])\n",
    "a = ax[0][1].imshow(np.reshape(log_sep[0].features[IDX * 2], log_sep[0].feature_shape[IDX * 2])[:, :41].T, aspect='auto')\n",
    "ax[0][1].set_title(log_sep[0].label[IDX * 2])\n",
    "plt.colorbar(a, ax=ax[0][1])\n",
    "a = ax[0][2].imshow(np.reshape(log_sep[0].features[IDX * 2 + 1], log_sep[0].feature_shape[IDX * 2 + 1])[:, :41].T, aspect='auto')\n",
    "ax[0][2].set_title(log_sep[0].label[IDX * 2 + 1])\n",
    "plt.colorbar(a, ax=ax[0][2])\n",
    "a = ax[1][0].imshow(np.reshape(spk_mix[0].features[IDX], spk_mix[0].feature_shape[IDX]).T, aspect='auto')\n",
    "ax[1][0].set_title(spk_mix[0].label[IDX])\n",
    "plt.colorbar(a, ax=ax[1][0])\n",
    "a = ax[1][1].imshow(np.reshape(spk_sep[0].features[IDX * 2], spk_sep[0].feature_shape[IDX * 2]).T, aspect='auto')\n",
    "ax[1][1].set_title(spk_sep[0].label[IDX * 2])\n",
    "plt.colorbar(a, ax=ax[1][1])\n",
    "a = ax[1][2].imshow(np.reshape(spk_sep[0].features[IDX * 2 + 1], spk_sep[0].feature_shape[IDX * 2 + 1]).T, aspect='auto')\n",
    "ax[1][2].set_title(spk_sep[0].label[IDX * 2 + 1])\n",
    "plt.colorbar(a, ax=ax[1][2])\n",
    "\n",
    "a = ax[2][0].imshow(np.reshape(log_mix[0].features[IDX], log_mix[0].feature_shape[IDX])[:, :41].T, aspect='auto')\n",
    "ax[2][0].set_title(log_mix[0].label[IDX])\n",
    "plt.colorbar(a, ax=ax[2][0])\n",
    "a = ax[2][1].imshow(np.reshape(log_prj[0].features[IDX * 2], log_prj[0].feature_shape[IDX * 2]).T, aspect='auto')\n",
    "ax[2][1].set_title(log_prj[0].label[IDX * 2])\n",
    "plt.colorbar(a, ax=ax[2][1])\n",
    "a = ax[2][2].imshow(np.reshape(log_prj[0].features[IDX * 2 + 1], log_prj[0].feature_shape[IDX * 2 + 1]).T, aspect='auto')\n",
    "ax[2][2].set_title(log_prj[0].label[IDX * 2 + 1])\n",
    "plt.colorbar(a, ax=ax[2][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'train'\n",
    "h5 = tables.open_file('/Data/DATASETS/CAESAR_TIDIGITS/tidigits_log_sep.h5', 'r')\n",
    "log_sep = h5.list_nodes(os.path.join(os.sep, 'default', subset))\n",
    "h5 = tables.open_file('/Data/DATASETS/CAESAR_TIDIGITS/tidigits_log_mix.h5', 'r')\n",
    "log_mix = h5.list_nodes(os.path.join(os.sep, 'default', subset))\n",
    "h5 = tables.open_file('/Data/DATASETS/CAESAR_TIDIGITS/tidigits_spk_sep.h5', 'r')\n",
    "spk_sep = h5.list_nodes(os.path.join(os.sep, 'default', subset))\n",
    "h5 = tables.open_file('/Data/DATASETS/CAESAR_TIDIGITS/tidigits_spk_mix.h5', 'r')\n",
    "spk_mix = h5.list_nodes(os.path.join(os.sep, 'default', subset))\n",
    "\n",
    "for IDX in range(8000):\n",
    "    if not list(spk_sep[0].label[IDX // 2 * 2]) + list(spk_sep[0].label[IDX // 2 * 2 + 1]) == list(spk_mix[0].label[IDX // 2]):\n",
    "    #         print \"{} vs {}\".format(spk_sep[0].feature_shape[IDX][0], log_sep[0].feature_shape[IDX][0])\n",
    "        print \"SPK {}: {} // {} // {}\".format(IDX // 2, spk_sep[0].label[IDX // 2 * 2], spk_sep[0].label[IDX // 2 * 2 + 1], spk_mix[0].label[IDX // 2])\n",
    "        print \"LOG {}: {} // {} // {}\".format(IDX // 2, log_sep[0].label[IDX // 2 * 2], log_sep[0].label[IDX // 2 * 2 + 1], log_mix[0].label[IDX // 2])\n",
    "        print \"===\" * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'train'\n",
    "\n",
    "h5 = tables.open_file('/Data/DATASETS/CAESAR_TIDIGITS/tidigits_spk_sep_777_v07.h5', 'r')\n",
    "spk_sep = h5.list_nodes(os.path.join(os.sep, 'default', subset))\n",
    "\n",
    "IDX = 50\n",
    "\n",
    "# print spk_sep[0].label[IDX * 2]\n",
    "# print spk_sep[0].label[IDX * 2 + 1]\n",
    "\n",
    "\n",
    "# f, ax = plt.subplots(1, 2, figsize=(20,10))\n",
    "\n",
    "# a = ax[0].imshow(np.reshape(spk_sep[0].features[IDX * 2], spk_sep[0].feature_shape[IDX * 2]).T, aspect='auto')\n",
    "# ax[0].set_title(spk_sep[0].label[IDX * 2])\n",
    "# plt.colorbar(a, ax=ax[0])\n",
    "# a = ax[1].imshow(np.reshape(spk_sep[0].features[IDX * 2 + 1], spk_sep[0].feature_shape[IDX * 2 + 1]).T, aspect='auto')\n",
    "# ax[1].set_title(spk_sep[0].label[IDX * 2 + 1])\n",
    "# plt.colorbar(a, ax=ax[1])\n",
    "\n",
    "all_ = []\n",
    "for IDX in range(4000):\n",
    "    ll = spk_sep[0].label[IDX]\n",
    "    all_.extend(ll)\n",
    "\n",
    "print len(all_) \n",
    "count = 0\n",
    "for i in all_:\n",
    "    if i == 1:\n",
    "        count += 1\n",
    "\n",
    "print count * 1.0 / len(all_) * 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6270e1473994377bfaaac73da6ac7f6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-6.0\n",
      "1.1801807\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "subset = 'test'\n",
    "\n",
    "h5 = tables.open_file('/Data/DATASETS/CAESAR_TIDIGITS/tidigits_spk_sep_77_v07.h5', 'r')\n",
    "spk_sep = h5.list_nodes(os.path.join(os.sep, 'default', subset))\n",
    "\n",
    "max_ = 0.\n",
    "min_ = 1e20\n",
    "\n",
    "for idx in tqdm(range(4000)):\n",
    "    max_ = max(np.max(spk_sep[0].features[idx]), max_)\n",
    "    min_ = min(np.min(spk_sep[0].features[idx]), min_)\n",
    "    \n",
    "print min_\n",
    "print max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

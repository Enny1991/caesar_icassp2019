{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import es_utils as es\n",
    "from utils import itd_utils as itd\n",
    "from utils import prob_utils as prob\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from python_speech_features import logfbank\n",
    "\n",
    "from scipy.io import wavfile\n",
    "import re\n",
    "from scipy import stats\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import norm\n",
    "import gammamix\n",
    "\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import os\n",
    "import tables\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import load_model\n",
    "import sys\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization, LSTM, Bidirectional, TimeDistributed\n",
    "from keras.regularizers import l2\n",
    "\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_trigger(timestamps, addresses):\n",
    "    dx = 0.000001\n",
    "    idx =  np.argmax(np.diff(timestamps)/dx)\n",
    "    return timestamps[idx + 1:], addresses[idx + 1:]\n",
    "\n",
    "def wind_sp(ts, ch, w=0.001, limit=False, noise=False):\n",
    "    ts_int = (ts // w).astype('int32')\n",
    "    if len(ts_int) < 1:\n",
    "        ts_int = np.array([0,0])\n",
    "\n",
    "    A = np.zeros((np.max(ts_int) + 3000, 64))\n",
    "    if noise:\n",
    "        A += np.abs(np.random.randn(A.shape[0], A.shape[1]))\n",
    "\n",
    "    for _t, _c in zip(ts_int, ch):\n",
    "        A[_t, _c] += 1\n",
    "        \n",
    "    if limit:\n",
    "        A = np.minimum(A, np.ones_like(A))\n",
    "    # returning [ch, T]\n",
    "    return A.T\n",
    "\n",
    "def exp_feat(A, win=0.05, tpe='lap', l=300):\n",
    "    if tpe == 'exp':\n",
    "        t = np.arange(0, win, 0.001)\n",
    "        b = np.exp(-l * t)\n",
    "    elif tpe == 'lap':\n",
    "        t = np.arange(-win, win, 0.001)\n",
    "        b = np.exp(-l * np.abs(t))\n",
    "        \n",
    "    b /= np.linalg.norm(b)\n",
    "    AA = np.array([np.convolve(_a, b, 'same') for _a in A])\n",
    "    return AA\n",
    "\n",
    "def smear_fr(A, win=3, l=10):\n",
    "#     t = np.arange(-win, win, 1)\n",
    "#     b = np.exp(-l * np.abs(t))\n",
    "#     b = np.concatenate([np.arange(win), np.arange(win)[::-1]])\n",
    "#     A = np.array([np.convolve(_a, b, 'same') for _a in A.T])\n",
    "    A = gaussian_filter(A, 1)\n",
    "    return A\n",
    "\n",
    "def simple_low_pass(X, win=60, shift=40):\n",
    "    n_win = (X.shape[0] - win) // shift\n",
    "    XX = np.zeros((n_win, X.shape[1]))\n",
    "    for i in range(0, n_win):\n",
    "        XX[i] = np.sum(X[i * shift: (i + 1) * shift] , 0) / win\n",
    "    return XX\n",
    "\n",
    "def spike_features(T1, C1, limit=False, noise=False):\n",
    "    A = wind_sp(np.array(T1), np.array(C1).astype('int32'), limit=limit, noise=noise)\n",
    "    A = exp_feat(A, win=0.05, l=300, tpe='lap')\n",
    "    A = smear_fr(A, win=3)\n",
    "    A = np.log10(A + 1e-9)\n",
    "    A = simple_low_pass(A.T).T\n",
    "    return A\n",
    "\n",
    "def get_ctx_win(X, Y, ctx=5, shift=1):\n",
    "    n_win = (X.shape[1] - ctx * 2) // shift\n",
    "    X_re = np.array([X[:, i - ctx:i + ctx + 1].reshape(-1,) for i in range(ctx, n_win * shift, shift)])\n",
    "    Y_re = Y[:, ctx + 1:-(ctx * 2 - 1)].T\n",
    "    return X_re, Y_re\n",
    "\n",
    "\n",
    "def get_ctx_win2(X, Y, ctx=30, shift=1):\n",
    "    n_win = (X.shape[1] - ctx) // shift\n",
    "    X_re = np.array([X[:, i:i + ctx] for i in range(ctx, n_win * shift, shift)])\n",
    "    Y_re = np.array([Y[:, i:i + ctx] for i in range(ctx, n_win * shift, shift)])\n",
    "    return X_re.transpose(0, 2, 1), Y_re.transpose(0, 2, 1)\n",
    "\n",
    "\n",
    "def find_local_maxima(x, win=5):\n",
    "    to_pad = win // 2\n",
    "    x = np.pad(x, (to_pad,to_pad), 'constant', constant_values=(0, 0))\n",
    "    idx = []\n",
    "    for i in range(to_pad, len(x) - to_pad):\n",
    "        if np.argmax(x[i - to_pad: i + to_pad]) == to_pad:\n",
    "            idx.append(i - to_pad)\n",
    "    return np.array(idx)\n",
    "\n",
    "def correct_pos(pos):\n",
    "    if pos == 0:\n",
    "        pos = 1\n",
    "    if pos == 18:\n",
    "        pos = 17\n",
    "    return pos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sps(idx, subset, sigma=20, channels=np.arange(64), adj=-1, prior='', verbose=False):\n",
    "    \n",
    "    filename = basedir + 'aedat/{}/'.format(subset) + ALL_NAMES[subset][IDX] + '.aedat'\n",
    "    \n",
    "    # load / trigger / decode\n",
    "    timestamps, addresses = es.loadaerdat(filename)\n",
    "    timestamps, addresses = remove_trigger(timestamps, addresses)\n",
    "    timestamps, channel_id, ear_id, neuron_id, filterbank_id = es.decode_ams1c(timestamps, addresses, return_type=False)\n",
    "\n",
    "    # load wavs\n",
    "    spk1, spk2, _ = re.compile('[0-9]+').split(ALL_NAMES[subset][IDX])\n",
    "    _, utt1, utt2 = re.compile('[A-Z]+').split(ALL_NAMES[subset][IDX])\n",
    "    \n",
    "    angles = ALL_ANGLES[subset][IDX].split(' ')\n",
    "    pos1, pos2 = int(float(angles[1]) // 10), int(float(angles[2]) // 10)\n",
    "    pos1, pos2 = correct_pos(pos1), correct_pos(pos2)\n",
    "    pos = [pos1, pos2]\n",
    "    \n",
    "    fs, w1 = wavfile.read(basedir + 'wavs/{}/'.format(subset) + GTS1[subset][IDX].replace('\\\\', '/') + '.wav')\n",
    "    fs, w2 = wavfile.read(basedir + 'wavs/{}/'.format(subset) + GTS2[subset][IDX].replace('\\\\', '/') + '.wav')\n",
    "    \n",
    "    # get itd and fix them\n",
    "    total_evs = 0\n",
    "    _aa = {}\n",
    "    idx_dict = {}\n",
    "    for kk in channels:\n",
    "        indices_channels = np.isin(channel_id, np.array([kk]))\n",
    "        if kk in DICT_C:\n",
    "            try:\n",
    "                \n",
    "                _itds, _itd_idx = itd.get_itds(timestamps[indices_channels], ear_id[indices_channels], neuron_id[indices_channels], save_to_file=None,\n",
    "                                     verbose=False, max_itd=max_itd)\n",
    "                idx_dict[kk] = _itd_idx\n",
    "                total_evs += len(_itds)\n",
    "                _mu = DICT_C[kk]['mu']\n",
    "                _corr = (_itds - _mu)\n",
    "                _aa[kk] = _corr\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        else:\n",
    "            _itds, _itd_idx = itd.get_itds(timestamps[indices_channels], ear_id[indices_channels], neuron_id[indices_channels], save_to_file=None,\n",
    "                                     verbose=False, max_itd=max_itd)\n",
    "            _aa[kk] = _itds\n",
    "            idx_dict[kk] = _itd_idx\n",
    "            total_evs += len(_itds)\n",
    "    if verbose:\n",
    "        print \"#ENVS {} => RECOVERED {}\".format(len(timestamps), total_evs)\n",
    "    # get probabilities\n",
    "    index_angles = np.vstack([np.arange(19), np.arange(0, 190, 10)]).T\n",
    "\n",
    "    num_angles = len(PRIORS)\n",
    "    initial_estimate = np.ones(num_angles) / num_angles # all angles are a priori equally likely\n",
    "    transition_probabilities = prob.get_transition_probabilities(index_angles, sigma=sigma) # gaussian a priori probability of itds given a certain position\n",
    "    \n",
    "    arg = {}\n",
    "    all_itds = []\n",
    "    \n",
    "    for k, _a in _aa.iteritems():\n",
    "        estimates, argmax_estimates = prob.estimate(_a, initial_estimate, transition_probabilities, ITD_DICT, PRIORS)\n",
    "        arg[k] = argmax_estimates\n",
    "        all_itds.extend(_a)\n",
    "        \n",
    "    amax = np.array(list(itertools.chain.from_iterable([v for _, v in arg.iteritems()])))\n",
    "    \n",
    "    if verbose:\n",
    "        print \"{} vs {}\".format(len(all_itds), len(amax))\n",
    "    # get separated spikes\n",
    "    T = []\n",
    "    C = []\n",
    "    for p in [pos1, pos2]:\n",
    "        T1 = []\n",
    "        C1 = []\n",
    "        for ch in idx_dict.keys():\n",
    "            t = (timestamps[channel_id == ch])[idx_dict[ch][arg[ch] == p]]\n",
    "            T1.extend(t)\n",
    "            C1.extend(np.ones_like(t) * ch)\n",
    "            if adj>0:\n",
    "                for i in range(adj):\n",
    "                    t = (timestamps[channel_id == ch])[idx_dict[ch][arg[ch] == p - i]]\n",
    "                    T1.extend(t)\n",
    "                    C1.extend(np.ones_like(t) * ch)\n",
    "                    t = (timestamps[channel_id == ch])[idx_dict[ch][arg[ch] == p + i]]\n",
    "                    T1.extend(t)\n",
    "                    C1.extend(np.ones_like(t) * ch)\n",
    "        T.append(T1)\n",
    "        C.append(C1)\n",
    "    if verbose:\n",
    "        print \"#ENVS 1 {} ({})\".format(len(T[0]), len(C[0]))\n",
    "        print \"#ENVS 2 {} ({})\".format(len(T[1]), len(C[1]))\n",
    "    return T, C, w1, w2, fs, amax, pos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sps2(idx, subset, sigma=20, channels=np.arange(64), adj=False, prior='_mf', verbose=False):\n",
    "    \n",
    "    \n",
    "    filename = basedir + 'aedat/{}/'.format(subset) + ALL_NAMES[subset][IDX] + '.aedat'\n",
    "    \n",
    "    # load / trigger / decode\n",
    "    timestamps, addresses = es.loadaerdat(filename)\n",
    "    timestamps, addresses = remove_trigger(timestamps, addresses)\n",
    "    timestamps, channel_id, ear_id, neuron_id, filterbank_id = es.decode_ams1c(timestamps, addresses, return_type=False)\n",
    "\n",
    "    # load wavs\n",
    "    spk1, spk2, _ = re.compile('[0-9]+').split(ALL_NAMES[subset][IDX])\n",
    "    _, utt1, utt2 = re.compile('[A-Z]+').split(ALL_NAMES[subset][IDX])\n",
    "    \n",
    "    angles = ALL_ANGLES[subset][IDX].split(' ')\n",
    "    pos1, pos2 = int(float(angles[1]) // 10), int(float(angles[2]) // 10)\n",
    "    pos1, pos2 = correct_pos(pos1), correct_pos(pos2)\n",
    "    pos = [pos1, pos2]\n",
    "    \n",
    "    fs, w1 = wavfile.read(basedir + 'wavs/{}/'.format(subset) + GTS1[subset][IDX].replace('\\\\', '/') + '.wav')\n",
    "    fs, w2 = wavfile.read(basedir + 'wavs/{}/'.format(subset) + GTS2[subset][IDX].replace('\\\\', '/') + '.wav')\n",
    "    \n",
    "    # get itd and fix them\n",
    "    total_evs = 0\n",
    "    _aa = {}\n",
    "    idx_dict = {}\n",
    "    for kk in channels:\n",
    "        indices_channels = np.isin(channel_id, np.array([kk]))\n",
    "        if kk in DICT_C:\n",
    "            try:\n",
    "                total_evs += len(timestamps[indices_channels])\n",
    "                _itds, _itd_idx = itd.get_itds(timestamps[indices_channels], ear_id[indices_channels], neuron_id[indices_channels], save_to_file=None,\n",
    "                                     verbose=False, max_itd=max_itd)\n",
    "#                 print \"CH: {} => #EVS = {}\".format(kk, len(_itds))\n",
    "                idx_dict[kk] = _itd_idx\n",
    "                _mu = DICT_C[kk]['mu']\n",
    "                _corr = (_itds - _mu)\n",
    "                _aa[kk] = _corr\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    if verbose:\n",
    "        print \"#ENVS {} => RECOVERED {}\".format(len(timestamps), total_evs)\n",
    "    # get probabilities\n",
    "    index_angles = np.vstack([np.arange(19), np.arange(0, 190, 10)]).T\n",
    "    num_angles = len(PRIORS)\n",
    "    initial_estimate = np.ones(num_angles) / num_angles # all angles are a priori equally likely\n",
    "    \n",
    "    all_T = []\n",
    "    \n",
    "    for i in tqdm(range(1, 180, 5)):\n",
    "    \n",
    "        transition_probabilities = prob.get_transition_probabilities(index_angles, sigma=i) # gaussian a priori probability of itds given a certain position\n",
    "\n",
    "        arg = {}\n",
    "        for k, _a in _aa.iteritems():\n",
    "            estimates, argmax_estimates = prob.estimate(_a, initial_estimate, transition_probabilities, ITD_DICT, PRIORS)\n",
    "            arg[k] = argmax_estimates\n",
    "\n",
    "        amax = np.array(list(itertools.chain.from_iterable([v for _, v in arg.iteritems()])))\n",
    "\n",
    "        T = []\n",
    "        C = []\n",
    "        for p in [pos1, pos2]:\n",
    "            T1 = []\n",
    "            C1 = []\n",
    "            for ch in idx_dict.keys():\n",
    "                t = (timestamps[channel_id == ch])[idx_dict[ch][arg[ch] == p]]\n",
    "                T1.extend(t)\n",
    "                C1.extend(np.ones_like(t) * ch)\n",
    "                if adj:\n",
    "                    t = (timestamps[channel_id == ch])[idx_dict[ch][arg[ch] == p - 1]]\n",
    "                    T1.extend(t)\n",
    "                    C1.extend(np.ones_like(t) * ch)\n",
    "                    t = (timestamps[channel_id == ch])[idx_dict[ch][arg[ch] == p + 1]]\n",
    "                    T1.extend(t)\n",
    "                    C1.extend(np.ones_like(t) * ch)\n",
    "            T.append(T1)\n",
    "            C.append(C1)\n",
    "        all_T.append(T)\n",
    "    return all_T\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = '/Data/DATASETS/CAESAR_TIDIGITS/'\n",
    "\n",
    "num_bins = 80\n",
    "max_itd = 800e-6\n",
    "num_channels = 64\n",
    "\n",
    "ALL_ANGLES = {}\n",
    "ALL_NAMES = {}\n",
    "\n",
    "VALID_IDX = {'train': [], 'test': []}\n",
    "GTS1 = {}\n",
    "GTS2 = {}\n",
    "\n",
    "with open(basedir + 'log_train.txt') as t:\n",
    "    ALL_ANGLES['train'] = [x for x in t.readlines()]\n",
    "\n",
    "with open(basedir + 'log_test.txt') as t:\n",
    "    ALL_ANGLES['test'] = [x for x in t.readlines()]\n",
    "\n",
    "with open('train_set.txt') as t:\n",
    "    ALL_NAMES['train'] = [line.split(' ')[4].strip() for line in t.readlines()]\n",
    "with open('train_set.txt') as t:\n",
    "    GTS1['train'] = [line.split(' ')[0].strip() for line in t.readlines()]\n",
    "with open('train_set.txt') as t:\n",
    "    GTS2['train'] = [line.split(' ')[1].strip() for line in t.readlines()]\n",
    "    \n",
    "with open('test_set.txt') as t:\n",
    "    ALL_NAMES['test'] = [line.split(' ')[4].strip() for line in t.readlines()]\n",
    "with open('test_set.txt') as t:\n",
    "    GTS1['test'] = [line.split(' ')[0].strip() for line in t.readlines()]\n",
    "with open('test_set.txt') as t:\n",
    "    GTS2['test'] = [line.split(' ')[1].strip() for line in t.readlines()]\n",
    "    \n",
    "PRIORS = np.load('priors_long_hv_corrected_all.npy')\n",
    "DICT_C = pkl.load(open('correction_dict.pkl', 'r'))\n",
    "ITD_DICT = itd.get_itd_dict(max_itd, num_bins) \n",
    "\n",
    "for subset in ['train', 'test']:\n",
    "    for idx, l in enumerate(ALL_ANGLES[subset]):\n",
    "        spk1, spk2, _ = re.compile('[0-9]+').split(l.split(' ')[0])\n",
    "        pos1, pos2 = int(float(l.split(' ')[1]) // 10) , int(float(l.split(' ')[2]) // 10)\n",
    "        pos1, pos2 = correct_pos(pos1), correct_pos(pos2)\n",
    "        if np.abs(pos1 - pos2) > 1:\n",
    "            VALID_IDX[subset].append(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "SPK_DICT = {}\n",
    "\n",
    "count = 0\n",
    "with open('train_set.txt') as t:\n",
    "    s = [(line.split(' ')[0].strip(), line.split(' ')[0].strip()) for line in t.readlines()]\n",
    "    for i, (s1, s2) in enumerate(s):\n",
    "        k = s1.split('\\\\')[4].strip()\n",
    "        if k not in SPK_DICT:\n",
    "            SPK_DICT[k] = count\n",
    "            count += 1\n",
    "        k = s2.split('\\\\')[4].strip()\n",
    "        if k not in SPK_DICT:\n",
    "            SPK_DICT[k] = count\n",
    "            count += 1\n",
    "with open('test_set.txt') as t:\n",
    "    s = [(line.split(' ')[0].strip(), line.split(' ')[0].strip()) for line in t.readlines()]\n",
    "    for i, (s1, s2) in enumerate(s):\n",
    "        k = s1.split('\\\\')[4].strip()\n",
    "        if k not in SPK_DICT:\n",
    "            SPK_DICT[k] = count\n",
    "            count += 1\n",
    "        k = s2.split('\\\\')[4].strip()\n",
    "        if k not in SPK_DICT:\n",
    "            SPK_DICT[k] = count\n",
    "            count += 1\n",
    "\n",
    "print len(SPK_DICT.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_SPK = {'train': {'s1': [], 's2': []}, 'test': {'s1': [], 's2': []}}\n",
    "\n",
    "for subset in ['train', 'test']:\n",
    "    for idx, l in enumerate(ALL_ANGLES[subset]):\n",
    "        spk1, spk2, _ = re.compile('[0-9]+').split(l.split(' ')[0])\n",
    "        ALL_SPK[subset]['s1'].append(SPK_DICT[spk1])\n",
    "        ALL_SPK[subset]['s2'].append(SPK_DICT[spk2])\n",
    "\n",
    "ALL_LBL = {'train': [], 'test': []}\n",
    "\n",
    "for subset in ['train', 'test']:\n",
    "    for i in VALID_IDX[subset]:\n",
    "        ALL_LBL[subset].append(ALL_SPK[subset]['s1'][i])\n",
    "        ALL_LBL[subset].append(ALL_SPK[subset]['s2'][i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 301\n",
    "subset = 'train'\n",
    "\n",
    "ch = np.arange(0,64)\n",
    "print \"normal prior\"\n",
    "T, C, w1, w2, fs, amax, pos = sps(IDX, subset, sigma=3, channels=ch, adj=2, prior='', verbose=True)\n",
    "spkf1 = spike_features(T[0], C[0], limit=True, noise=False)[2:]\n",
    "spkf2 = spike_features(T[1], C[1], limit=True, noise=False)[2:]\n",
    "\n",
    "print \"mf prior\"\n",
    "T, C, w3, w4, fs, amax, pos = sps(IDX, subset, sigma=3, channels=ch, adj=2, prior='_mf', verbose=True)\n",
    "spkf3 = spike_features(T[0], C[0], limit=True, noise=False)[2:]\n",
    "spkf4 = spike_features(T[1], C[1], limit=True, noise=False)[2:]\n",
    "# w_spk1 = get_ctx_win(spkf1[::-1])\n",
    "# w_spk2 = get_ctx_win(spkf2[::-1])\n",
    "\n",
    "f, ax = plt.subplots(2, 4,figsize=(20, 5))\n",
    "\n",
    "for spkf, k, w in zip([spkf1, spkf2, spkf3, spkf4], range(4), [w1, w2, w3, w4]):\n",
    "    \n",
    "    Y = logfbank(w, fs, nfft=2048,winlen=0.06,winstep=0.04)\n",
    "    ax[0][k].imshow(Y.T, aspect='auto')\n",
    "    ax[1][k].imshow(spkf[:, :Y.shape[0]], aspect='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'test'\n",
    "t_idx = {}\n",
    "for IDX in range(1000):\n",
    "    print \"Doing {} ...\".format(IDX)\n",
    "    all_T = sps2(IDX, subset, sigma=i, channels=ch, adj=True)\n",
    "    t_idx[IDX] = all_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(t_idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxes = []\n",
    "basis = []\n",
    "for k, v in t_idx.iteritems():\n",
    "    _one = [len(t[0]) for t in v]\n",
    "    _two = [len(t[1]) for t in v]\n",
    "    maxes.append(np.max(_one))\n",
    "    basis.append(np.argmax(_one))\n",
    "    maxes.append(np.max(_two))\n",
    "    basis.append(np.argmax(_two))\n",
    "    \n",
    "print len(maxes)\n",
    "print len(basis)\n",
    "\n",
    "xx = sns.kdeplot(basis, maxes, cmap=\"Greens\", shade=True, shade_lowest=True)\n",
    "xx.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.arange(0, 200, 10)\n",
    "a = plt.hist(amax, int(np.max(amax)), normed=True)\n",
    "\n",
    "b = np.linspace(0, 19, 19)\n",
    "\n",
    "gmm = GaussianMixture(2, n_init=20)\n",
    "gmm.fit(amax.reshape(-1,1))\n",
    "plt.figure()\n",
    "g1 = norm.pdf(b, gmm.means_[0], gmm.covariances_[0][0])\n",
    "g2 = norm.pdf(b, gmm.means_[1], gmm.covariances_[1][0])\n",
    "# g3 = norm.pdf(b, gmm.means_[2], gmm.covariances_[2][0])\n",
    "plt.plot(g1 / np.max(g1))\n",
    "plt.plot(g2 / np.max(g2))\n",
    "# plt.plot(g3 / np.max(g3))\n",
    "\n",
    "print A[np.argmax(g1)]\n",
    "print A[np.argmax(g2)]\n",
    "print pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check precision in localization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout.flush()\n",
    "acc = 0.\n",
    "count = 0.\n",
    "for IDX in tqdm(range(5999)):\n",
    "    pos1, pos2 = int(float(all_angles[IDX].split(' ')[1]) // 10) , int(float(all_angles[IDX].split(' ')[2]) // 10)\n",
    "#     if pos1 != pos2: \n",
    "    if np.abs(pos1 - pos2) >= 2:  # non-adiacent or same spot\n",
    "        p = np.sort([correct_pos(pos1), correct_pos(pos2)])\n",
    "        ch = np.arange(15,30)\n",
    "        T, C, w1, w2, fs, amax = sps(basedir + 'aedat/train/' + all_train[IDX] + '.aedat', sigma=5, channels=ch)\n",
    "        a = np.histogram(amax, 19, range=[0, 18], normed=True)\n",
    "        ii = find_local_maxima(a[0])\n",
    "        \n",
    "        kk = np.sort(ii[np.argsort(a[0][ii])[::-1][:2]])\n",
    "        if len(kk) == 2:\n",
    "            if np.abs(p[0] - kk[0]) <= 1 and np.abs(p[1] - kk[1]) <= 1:\n",
    "                acc += 1.\n",
    "            count += 1.\n",
    "print acc / count * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many are same position or adiacent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout.flush()\n",
    "idx_0_train = []\n",
    "idx_1_train = []\n",
    "idx_0_test = []\n",
    "idx_1_test = []\n",
    "acc = 0.\n",
    "count = 0.\n",
    "for IDX in range(1999):\n",
    "    pos1, pos2 = int(float(all_angles_test[IDX].split(' ')[1]) // 10) , int(float(all_angles_test[IDX].split(' ')[2]) // 10)\n",
    "    pos1, pos2 = correct_pos(pos1), correct_pos(pos2)\n",
    "    if pos1 == pos2:\n",
    "        count += 1.\n",
    "        idx_0_test.append(IDX)\n",
    "\n",
    "print \"TEST: same angle in {:.2} % of the cases ({})\".format(count / 1999 * 100, len(idx_0_test))\n",
    "\n",
    "count = 0.\n",
    "for IDX in range(5999):\n",
    "    pos1, pos2 = int(float(all_angles[IDX].split(' ')[1]) // 10) , int(float(all_angles[IDX].split(' ')[2]) // 10)\n",
    "    pos1, pos2 = correct_pos(pos1), correct_pos(pos2)\n",
    "    if pos1 == pos2:\n",
    "        count += 1.\n",
    "        idx_0_train.append(IDX)\n",
    "        \n",
    "print \"TRAIN: same angle in {:.3} % of the cases ({})\".format(count / 5999 * 100, len(idx_0_train))\n",
    "\n",
    "sys.stdout.flush()\n",
    "acc = 0.\n",
    "count = 0.\n",
    "for IDX in range(1999):\n",
    "    \n",
    "    pos1, pos2 = int(float(all_angles_test[IDX].split(' ')[1]) // 10) , int(float(all_angles_test[IDX].split(' ')[2]) // 10)\n",
    "    pos1, pos2 = correct_pos(pos1), correct_pos(pos2)\n",
    "    if np.abs(pos1 - pos2) == 1:\n",
    "        count += 1.\n",
    "        idx_1_test.append(IDX)\n",
    "\n",
    "print \"TEST: adiacent angle in {:.2} % of the cases ({})\".format(count / 1999 * 100, len(idx_1_test))\n",
    "\n",
    "count = 0.\n",
    "for IDX in range(5999):\n",
    "    pos1, pos2 = int(float(all_angles[IDX].split(' ')[1]) // 10) , int(float(all_angles[IDX].split(' ')[2]) // 10)\n",
    "    pos1, pos2 = correct_pos(pos1), correct_pos(pos2)\n",
    "    if np.abs(pos1 - pos2) == 1:\n",
    "        count += 1.\n",
    "        idx_1_train.append(IDX)\n",
    "        \n",
    "print \"TRAIN: adiacent angle in {:.3} % of the cases ({})\".format(count / 5999 * 100, len(idx_1_train))\n",
    "\n",
    "ang_diff_test = []\n",
    "\n",
    "for IDX in range(1999):\n",
    "    pos1, pos2 = int(float(all_angles_test[IDX].split(' ')[1]) // 10) , int(float(all_angles_test[IDX].split(' ')[2]) // 10)\n",
    "    pos1, pos2 = correct_pos(pos1), correct_pos(pos2)\n",
    "    ang_diff_test.append(np.abs(pos1 - pos2))\n",
    "    \n",
    "ang_diff_train = []\n",
    "\n",
    "for IDX in range(5999):\n",
    "    pos1, pos2 = int(float(all_angles[IDX].split(' ')[1]) // 10) , int(float(all_angles[IDX].split(' ')[2]) // 10)\n",
    "    pos1, pos2 = correct_pos(pos1), correct_pos(pos2)\n",
    "    ang_diff_train.append(np.abs(pos1 - pos2))\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "_ = ax.hist(ang_diff_train, 16, normed=True)\n",
    "ax.set_title('train')\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "_ = ax.hist(ang_diff_test, 16, normed=True)\n",
    "ax.set_title('test')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and shift correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 390\n",
    "subset = 'test'\n",
    "\n",
    "h5file = '/Data/DATASETS/CAESAR_TIDIGITS/tidigits_log_mix.h5'\n",
    "\n",
    "h5 = tables.open_file(h5file, 'r')\n",
    "dataset = h5.get_node(os.path.join(os.sep, 'default', subset))\n",
    "\n",
    "channels = h5.list_nodes(os.path.join(os.sep, 'default', subset))\n",
    "\n",
    "print ALL_NAMES[subset][IDX]\n",
    "timestamps, addresses = es.loadaerdat(basedir + 'aedat/{}/'.format(subset) + ALL_NAMES[subset][IDX] + '.aedat')\n",
    "timestamps, addresses = remove_trigger(timestamps, addresses)\n",
    "\n",
    "timestamps, channel_id, ear_id, neuron_id, filterbank_id = es.decode_ams1c(timestamps, addresses, return_type=False)\n",
    "\n",
    "# load wav\n",
    "spk1, spk2, _ = re.compile('[0-9]+').split(ALL_NAMES[subset][IDX])\n",
    "_, utt1, utt2 = re.compile('[A-Z]+').split(ALL_NAMES[subset][IDX])\n",
    "pos1, pos2 = int(float(ALL_ANGLES[subset][IDX].split(' ')[1]) // 10) , int(float(ALL_ANGLES[subset][IDX].split(' ')[2]) // 10)\n",
    "print pos1\n",
    "print pos2\n",
    "fs, w1 = wavfile.read(basedir + 'wavs/{}/'.format(subset) + GTS1[subset][IDX].replace('\\\\', '/') + '.wav')\n",
    "fs, w2 = wavfile.read(basedir + 'wavs/{}/'.format(subset) + GTS2[subset][IDX].replace('\\\\', '/') + '.wav')\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    " \n",
    "ax = fig.add_subplot(2,1,1)\n",
    "\n",
    "m_len = max([len(w1), len(w2)])\n",
    "w1 = np.concatenate([w1, np.zeros(m_len - len(w1),)])\n",
    "w2 = np.concatenate([w2, np.zeros(m_len - len(w2),)])\n",
    "\n",
    "print m_len\n",
    "\n",
    "Y = np.reshape(channels[0].features[IDX], channels[0].feature_shape[IDX])[:, :41]\n",
    "\n",
    "print Y.shape\n",
    "ax.imshow(Y.T, aspect='auto')\n",
    "# ax.plot(np.sum(Y, 1) / 400.0 * 20, 'r')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "ax = fig.add_subplot(2,1,2)\n",
    "\n",
    "K = spike_features(timestamps, channel_id)[2:]\n",
    "\n",
    "env_log = np.sum(Y, 1)\n",
    "env_sps = np.sum(K, 0)\n",
    "\n",
    "k = np.correlate(env_log, env_sps, 'full')\n",
    "\n",
    "shift = np.argmax(np.abs(k)) - len(env_sps)\n",
    "print shift\n",
    "\n",
    "ax.imshow(K[::-1, np.abs(shift):], aspect='auto')\n",
    "# ax.plot(np.sum(K, 0) / 1000.0 * 10 + 20, 'r')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# ax.plot(np.sum(K, 0) / 1000.0 * 25, 'r')\n",
    "# ax.imshow(np.log10(wind_sp(timestamps, channel_id) + 1e-9), aspect='auto')\n",
    "# ax.plot(timestamps, channel_id, 'o')\n",
    "ax.set_xlim([0, Y.shape[0]])\n",
    "# ax.plot()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(2,1,1)\n",
    "ax.set_xlim([0, Y.shape[0]])\n",
    "ax.plot(np.sum(Y, 1) / 400.0 * 20, 'r')\n",
    "ax = fig.add_subplot(2,1,2)\n",
    "ax.plot(np.sum(K, 0) / 1000.0 * 10 + 20, 'r')\n",
    "ax.set_xlim([0, Y.shape[0]])\n",
    "\n",
    "h5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 388\n",
    "subset = 'test'\n",
    "\n",
    "h5file = '/Data/DATASETS/CAESAR_TIDIGITS/tidigits_log_mix.h5'\n",
    "\n",
    "h5 = tables.open_file(h5file, 'r')\n",
    "dataset = h5.get_node(os.path.join(os.sep, 'default', subset))\n",
    "\n",
    "channels = h5.list_nodes(os.path.join(os.sep, 'default', subset))\n",
    "\n",
    "print ALL_NAMES[subset][IDX]\n",
    "timestamps, addresses = es.loadaerdat(basedir + 'aedat/{}/'.format(subset) + ALL_NAMES[subset][IDX] + '.aedat')\n",
    "plt.plot(timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Create dataset with alligned log/spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_log = {'train': [], 'test': []}\n",
    "all_spk = {'train': [], 'test': []}\n",
    "\n",
    "h5file = '/Data/DATASETS/CAESAR_TIDIGITS/tidigits_log_mix.h5'\n",
    "\n",
    "h5 = tables.open_file(h5file, 'r')\n",
    "\n",
    "channels = h5.list_nodes(os.path.join(os.sep, 'default', 'train'))\n",
    "\n",
    "for subset in ['train', 'test']:\n",
    "    print \"DOING {}\".format(subset)\n",
    "    channels = h5.list_nodes(os.path.join(os.sep, 'default', subset))\n",
    "    \n",
    "    for IDX in tqdm(range(len(ALL_NAMES[subset]))):\n",
    "        timestamps, addresses = es.loadaerdat(basedir + 'aedat/{}/'.format(subset) + ALL_NAMES[subset][IDX] + '.aedat')\n",
    "        timestamps, addresses = remove_trigger(timestamps, addresses)\n",
    "\n",
    "        timestamps, channel_id, ear_id, neuron_id, filterbank_id = es.decode_ams1c(timestamps, addresses, return_type=False)\n",
    "\n",
    "        Y = np.reshape(channels[0].features[IDX], channels[0].feature_shape[IDX])[:, :41]\n",
    "\n",
    "        K = spike_features(timestamps, channel_id)[2:]\n",
    "\n",
    "        env_log = np.sum(Y, 1)\n",
    "        env_sps = np.sum(K, 0)\n",
    "\n",
    "        k = np.correlate(env_log, env_sps, 'full')\n",
    "\n",
    "        shift = np.argmax(np.abs(k)) - len(env_sps)\n",
    "\n",
    "        K = K[:, np.abs(shift):]\n",
    "        K = K[:, :Y.shape[0]]\n",
    "        Y = Y.T\n",
    "\n",
    "        if K.shape[1] != Y.shape[1]:\n",
    "            print K.shape\n",
    "            print Y.shape\n",
    "            raise ValueError()\n",
    "        all_log[subset].append(Y)\n",
    "        all_spk[subset].append(K)\n",
    "\n",
    "\n",
    "h5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, 2, figsize=(10, 5))\n",
    "ax[0][0].imshow(all_spk['train'][3], aspect='auto')\n",
    "ax[1][0].imshow(all_log['train'][3], aspect='auto')\n",
    "ax[0][1].imshow(all_spk['test'][3], aspect='auto')\n",
    "ax[1][1].imshow(all_log['test'][3], aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump({'log': all_log['train'], 'spk': all_spk['train']}, open('all_log_spk_train_v02.pkl', 'w'))\n",
    "pkl.dump({'log': all_log['test'], 'spk': all_spk['test']}, open('all_log_spk_test_v02.pkl', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and train reconstruction model (MLP) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_v = ''  # default\n",
    "ds_v = '_v02'\n",
    "print \"TRAIN:\"\n",
    "all_mix_log = pkl.load(open('all_log_spk_train{}.pkl'.format(ds_v), 'r'))['log']\n",
    "print \"\\tMIX LOG => {}\".format(len(all_mix_log))\n",
    "all_mix_spk = pkl.load(open('all_log_spk_train{}.pkl'.format(ds_v), 'r'))['spk']\n",
    "print \"\\tMIX SPK => {}\".format(len(all_mix_spk))\n",
    "print \"TEST:\"\n",
    "all_mix_log_test = pkl.load(open('all_log_spk_test{}.pkl'.format(ds_v), 'r'))['log']\n",
    "print \"\\tMIX LOG => {}\".format(len(all_mix_log_test))\n",
    "all_mix_spk_test = pkl.load(open('all_log_spk_test{}.pkl'.format(ds_v), 'r'))['spk']\n",
    "print \"\\tSPK LOG => {}\".format(len(all_mix_spk_test))\n",
    "\n",
    "# I also use the samples from spk_sep train\n",
    "ds_v = '_v05'\n",
    "print \"TRAIN:\"\n",
    "all_sep_log = pkl.load(open('all_sep_spk_train{}.pkl'.format(ds_v), 'r'))['log']\n",
    "print \"\\tSEP LOG => {}\".format(len(all_sep_log))\n",
    "all_sep_spk = pkl.load(open('all_sep_spk_train{}.pkl'.format(ds_v), 'r'))['spk']\n",
    "print \"\\tSEP SPK => {}\".format(len(all_sep_spk))\n",
    "print \"TEST:\"\n",
    "all_sep_log_test = pkl.load(open('all_sep_spk_test{}.pkl'.format(ds_v), 'r'))['log']\n",
    "print \"\\tSEP LOG => {}\".format(len(all_sep_log_test))\n",
    "all_sep_spk_test = pkl.load(open('all_sep_spk_test{}.pkl'.format(ds_v), 'r'))['spk']\n",
    "print \"\\tSEP SPK => {}\".format(len(all_sep_spk_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for IDX in tqdm(range(20)):\n",
    "    f, ax = plt.subplots(2, 2, figsize=(20,10))\n",
    "\n",
    "    im = ax[0][0].imshow(all_sep_spk[IDX], aspect='auto')\n",
    "    plt.colorbar(im, ax=ax[0][0])\n",
    "    im = ax[1][0].imshow(all_sep_log[IDX], aspect='auto')\n",
    "    plt.colorbar(im, ax=ax[1][0])\n",
    "    im = ax[0][1].imshow(all_sep_spk_test[IDX], aspect='auto')\n",
    "    plt.colorbar(im, ax=ax[0][1])\n",
    "    im = ax[1][1].imshow(all_sep_log_test[IDX], aspect='auto')\n",
    "    plt.colorbar(im, ax=ax[1][1])\n",
    "\n",
    "    plt.savefig('./imgs/train/{}_v05.png'.format(IDX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to train I use the all mix plus the separated train\n",
    "# all_x_train = all_mix_spk + all_mix_spk_test + all_sep_spk\n",
    "# all_y_train = all_mix_log + all_mix_log_test + all_sep_log\n",
    "# or only the separated one\n",
    "all_x_train = all_sep_spk\n",
    "all_y_train = all_sep_log\n",
    "# or only the mixed one\n",
    "# all_x_train = all_mix_spk\n",
    "# all_y_train = all_mix_log\n",
    "\n",
    "all_x_test = all_sep_spk_test\n",
    "all_y_test = all_sep_log_test\n",
    "\n",
    "Y_train_pre = np.concatenate(all_y_train, 1)\n",
    "print Y_train_pre.shape\n",
    "X_train_pre = np.concatenate(all_x_train, 1)\n",
    "print X_train_pre.shape\n",
    "\n",
    "Y_test_pre = np.concatenate(all_y_test, 1)\n",
    "print Y_test_pre.shape\n",
    "X_test_pre = np.concatenate(all_x_test, 1)\n",
    "print X_test_pre.shape\n",
    "\n",
    "## LTD\n",
    "# Y_train_pre = Y_train_pre[:20]\n",
    "# Y_test_pre = Y_test_pre[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = 10\n",
    "shift = 1\n",
    "X_train, Y_train = get_ctx_win(X_train_pre, Y_train_pre, ctx=ctx, shift=shift)\n",
    "print X_train.shape\n",
    "print Y_train.shape\n",
    "\n",
    "X_test, Y_test = get_ctx_win(X_test_pre, Y_test_pre, ctx=ctx, shift=shift)\n",
    "print X_test.shape \n",
    "print Y_test.shape\n",
    "\n",
    "# try to normalize\n",
    "mean_x = np.mean(X_train, 0)\n",
    "std_x = np.std(X_train, 0)\n",
    "mean_y = np.mean(Y_train, 0)\n",
    "std_y = np.std(Y_train, 0)\n",
    "\n",
    "# X_train = np.log10(X_train + 1e-9)\n",
    "# X_test = np.log10(X_test + 1e-9)\n",
    "\n",
    "# Y_train = np.log10(Y_train + 1e-9)\n",
    "# Y_test = np.log10(Y_test + 1e-9)\n",
    "\n",
    "# visually, train vs test\n",
    "f, ax = plt.subplots(2, 2, figsize=(20,10))\n",
    "\n",
    "beg = 5200\n",
    "end = 5300\n",
    "\n",
    "ax[0][0].imshow(X_test.T[:, beg:end], aspect='auto')\n",
    "ax[1][0].imshow(Y_test.T[:, beg:end], aspect='auto')\n",
    "\n",
    "ax[0][1].imshow(X_train.T[:, beg:end], aspect='auto')\n",
    "ax[1][1].imshow(Y_train.T[:, beg:end], aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(Dense(1024, input_shape=(X_train.shape[1],)))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1024))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(Y_train.shape[1]))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "mc = keras.callbacks.ModelCheckpoint('./spk2log_mix_v05.h5', monitor='val_loss', save_best_only=True)\n",
    "results = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, verbose=True, batch_size=256, callbacks=[estop, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "plt.plot(results.history['loss'], label='Train loss')\n",
    "plt.plot(results.history['val_loss'], label='Test loss')\n",
    "plt.legend()\n",
    "model = load_model('spk2log_mix_v05.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visually, train vs test\n",
    "f, ax = plt.subplots(2, 2, figsize=(20,10))\n",
    "Y_pred_train = model.predict(X_train)\n",
    "Y_pred_test = model.predict(X_test)\n",
    "\n",
    "beg = 5200\n",
    "end = 5300\n",
    "\n",
    "ax[0][0].imshow(Y_test.T[:, beg:end], aspect='auto')\n",
    "ax[1][0].imshow(Y_pred_test.T[:, beg:end], aspect='auto')\n",
    "\n",
    "ax[0][1].imshow(Y_train.T[:, beg:end], aspect='auto')\n",
    "ax[1][1].imshow(Y_pred_train.T[:, beg:end], aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Regression v2 (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TS = 30\n",
    "X_train_lstm, Y_train_lstm = get_ctx_win2(X_train_pre, Y_train_pre, ctx=TS)\n",
    "X_test_lstm, Y_test_lstm = get_ctx_win2(X_test_pre, Y_test_pre, ctx=TS)\n",
    "\n",
    "print X_train_lstm.shape\n",
    "print Y_train_lstm.shape\n",
    "print X_test_lstm.shape\n",
    "print Y_test_lstm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.Sequential()\n",
    "model2.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])))\n",
    "model2.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model2.add(TimeDistributed(Dense(Y_train_lstm.shape[2])))\n",
    "model2.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "mc = keras.callbacks.ModelCheckpoint('./spk2log_mix_v03_lstm.h5', monitor='val_loss', save_best_only=True)\n",
    "results = model2.fit(X_train_lstm, Y_train_lstm, validation_data=(X_test_lstm, Y_test_lstm), epochs=100, verbose=True, batch_size=256, callbacks=[estop, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visually, train vs test\n",
    "f, ax = plt.subplots(2, 2, figsize=(20,10))\n",
    "Y_pred_train_lstm = model2.predict(X_train_lstm)\n",
    "Y_pred_test_lstm = model2.predict(X_test_lstm)\n",
    "\n",
    "beg = 5200\n",
    "end = 5300\n",
    "\n",
    "ax[0][0].imshow(Y_test_lstm[13].T, aspect='auto')\n",
    "ax[1][0].imshow(Y_pred_test_lstm[13].T, aspect='auto')\n",
    "\n",
    "ax[0][1].imshow(Y_train_lstm[13].T, aspect='auto')\n",
    "ax[1][1].imshow(Y_pred_train_lstm[13].T, aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLEARN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = MLPRegressor(hidden_layer_sizes=(256, 256), verbose=True, early_stopping=True)\n",
    "lr.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl.dump(lr, open('mlp_256_256_spk2log.pkl', 'wb'))\n",
    "lr = pkl.load(open('mlp_256_256_spk2log.pkl', 'r')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_train = lr.predict(X_train)\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(2,1,1)\n",
    "ax.imshow(Y_train.T[:, 1000:1200], aspect='auto')\n",
    "ax = fig.add_subplot(2,1,2)\n",
    "ax.imshow(Y_pred_train.T[:, 1000:1200], aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = lr.predict(X_test)\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(2,1,1)\n",
    "ax.imshow(Y_test.T[:, 1000:1200], aspect='auto')\n",
    "ax = fig.add_subplot(2,1,2)\n",
    "ax.imshow(Y_pred.T[:, 1000:1200], aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single sample \n",
    "regressor = model # lr\n",
    "IDX = 1001\n",
    "\n",
    "ex_x = all_sep_spk_test[IDX]\n",
    "ex_y = all_sep_log_test[IDX]\n",
    "print ex_x.shape \n",
    "print ex_y.shape\n",
    "\n",
    "ex_x, ex_y = get_ctx_win(ex_x, ex_y, ctx=3, shift=1)\n",
    "print ex_x.shape \n",
    "print ex_y.shape\n",
    "\n",
    "ex_pred = regressor.predict(ex_x)\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_subplot(3,1,1)\n",
    "ax.imshow(ex_y.T, aspect='auto')\n",
    "ax = fig.add_subplot(3,1,2)\n",
    "ax.imshow(ex_pred.T, aspect='auto')\n",
    "ax = fig.add_subplot(3,1,3)\n",
    "d1 = np.diff(ex_pred, axis=1)\n",
    "d2 = np.diff(d1, axis=1)\n",
    "deltas = np.concatenate([ex_pred[:, 2:], d1[:, 1:], d2], axis=1)\n",
    "ax.imshow(deltas.T, aspect='auto')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single sample \n",
    "regressor = model # lr\n",
    "ctx = 3\n",
    "shift = 1\n",
    "ds_v = '_v02'\n",
    "all_y = {'train': [], 'test': []}\n",
    "all_lbl = {'train': pkl.load(open('all_sep_spk_train{}.pkl'.format(ds_v), 'r'))['lbl'], 'test': pkl.load(open('all_sep_spk_test{}.pkl'.format(ds_v), 'r'))['lbl']} \n",
    "\n",
    "for task, x, y in zip(['train', 'test'], [all_sep_spk, all_sep_spk_test], [all_sep_log, all_sep_log_test]): \n",
    "    for ex_x, ex_y in tqdm(zip(x, y)):\n",
    "        ex_x, ex_y = get_ctx_win(ex_x, ex_y, ctx=ctx, shift=shift)\n",
    "\n",
    "        ex_pred = regressor.predict(ex_x)\n",
    "        all_y[task].append(ex_pred.T)\n",
    "\n",
    "print len(all_y['train'])\n",
    "print len(all_lbl['train'])\n",
    "print len(all_y['test'])\n",
    "print len(all_lbl['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check\n",
    "IDX = 1002\n",
    "f, ax = plt.subplots(2,2)\n",
    "ax[0][0].imshow(all_y['train'][IDX], aspect='auto')\n",
    "ax[1][0].imshow(all_sep_log[IDX], aspect='auto')\n",
    "ax[0][1].imshow(all_y['test'][IDX], aspect='auto')\n",
    "ax[1][1].imshow(all_sep_log_test[IDX], aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it\n",
    "ds_v = '_v02'\n",
    "pkl.dump({'spk': all_y['train'], 'lbl': all_lbl['train']}, open('all_log_prj_train{}.pkl'.format(ds_v), 'w'))\n",
    "pkl.dump({'spk': all_y['test'], 'lbl': all_lbl['test']}, open('all_log_prj_test{}.pkl'.format(ds_v), 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check what kind of error distribution there is in reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = model\n",
    "\n",
    "# Y_pred = regressor.predict(X_test)\n",
    "Q_test = (Y_pred_test - Y_test).reshape(-1)\n",
    "\n",
    "# Y_pred_train = regressor.predict(X_train)\n",
    "Q_train = (Y_pred_train - Y_train).reshape(-1)\n",
    "\n",
    "print Q_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2)\n",
    "# test error distribution\n",
    "\n",
    "a = ax[0].hist(Q_train, 100, normed=True)\n",
    "xt = a[1]  \n",
    "xmin, xmax = min(xt), max(xt)  \n",
    "lnspc = np.linspace(xmin, xmax, len(Q_train))\n",
    "\n",
    "# gaussian\n",
    "an, bn = stats.norm.fit(Q_train)  \n",
    "pdf_norm = stats.norm.pdf(lnspc, an, bn)  \n",
    "ax[0].plot(lnspc, pdf_norm, label=\"Gaussian\")\n",
    "\n",
    "# laplace\n",
    "al, bl = stats.laplace.fit(Q_train)  \n",
    "pdf_laplace = stats.laplace.pdf(lnspc, al, bl)  \n",
    "ax[0].plot(lnspc, pdf_laplace, label=\"Laplace\")\n",
    "\n",
    "# gamma\n",
    "ag, bg, cg = stats.gamma.fit(Q_train)  \n",
    "pdf_gamma = stats.gamma.pdf(lnspc, ag, bg, cg)  \n",
    "ax[0].plot(lnspc, pdf_gamma, label=\"Gamma\")\n",
    "\n",
    "ax[0].legend()\n",
    "\n",
    "a = ax[1].hist(Q_test, 100, normed=True)\n",
    "xt = a[1]  \n",
    "xmin, xmax = min(xt), max(xt)  \n",
    "lnspc = np.linspace(xmin, xmax, len(Q_test))\n",
    "\n",
    "# gaussian\n",
    "an, bn = stats.norm.fit(Q_test)  \n",
    "pdf_norm = stats.norm.pdf(lnspc, an, bn)  \n",
    "ax[1].plot(lnspc, pdf_norm, label=\"Gaussian\")\n",
    "\n",
    "# laplace\n",
    "al, bl = stats.laplace.fit(Q_test)  \n",
    "pdf_laplace = stats.laplace.pdf(lnspc, al, bl)  \n",
    "ax[1].plot(lnspc, pdf_laplace, label=\"Laplace\")\n",
    "\n",
    "# laplace\n",
    "ag, bg, cg = stats.gamma.fit(Q_test)  \n",
    "pdf_gamma = stats.gamma.pdf(lnspc, ag, bg, cg)  \n",
    "ax[1].plot(lnspc, pdf_gamma, label=\"Gamma\")\n",
    "\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = stats.norm.fit(Q_test)  \n",
    "pdf_norm = stats.norm.pdf(lnspc, mu, sigma)  \n",
    "plt.plot(lnspc, pdf_norm, label=\"Gaussian\")\n",
    "\n",
    "y = np.random.normal(size=100000) * sigma + mu\n",
    "_ = plt.hist(y, 100, normed=True, label='numpy', alpha=0.5)\n",
    "\n",
    "print mu\n",
    "print sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot normed histogram\n",
    "plt.hist(Q.reshape(-1), 100, normed=True)\n",
    "\n",
    "# find minimum and maximum of xticks, so we know\n",
    "# where we should compute theoretical distribution\n",
    "xt = plt.xticks()[0]  \n",
    "xmin, xmax = min(xt), max(xt)  \n",
    "lnspc = np.linspace(xmin, xmax, len(Q.reshape(-1)))\n",
    "# exactly same as above\n",
    "ag,bg = stats.laplace.fit(Q.reshape(-1))  \n",
    "pdf_laplace = stats.laplace.pdf(lnspc, ag, bg)  \n",
    "plt.plot(lnspc, pdf_laplace, label=\"Laplace\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single separation and reconstrucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 100\n",
    "subset = 'train'\n",
    "ctx = 5\n",
    "shift = 1\n",
    "\n",
    "T, C, w1, w2, fs, _, amax = sps(IDX, subset, sigma=5)\n",
    "\n",
    "spkf1 = spike_features(T[0], C[0])[2:]\n",
    "spkf2 = spike_features(T[1], C[1])[2:]\n",
    "\n",
    "ex_x = spkf1[::-1]\n",
    "ex_y = w1\n",
    "\n",
    "n_win = (ex_x.shape[1] - ctx * 2) // shift\n",
    "\n",
    "ex_x = np.array([ex_x[:, i - ctx:i + ctx + 1].reshape(-1,) for i in range(ctx, n_win * shift, shift)])\n",
    "print ex_x.shape \n",
    "ex_y = ex_y[:, ctx + 1:-(ctx * 2 - 1)].T\n",
    "print ex_y.shape\n",
    "\n",
    "\n",
    "ex_pred = lr.predict(ex_x)\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(2,1,1)\n",
    "ax.imshow(ex_y.T, aspect='auto')\n",
    "ax = fig.add_subplot(2,1,2)\n",
    "ax.imshow(ex_pred.T, aspect='auto')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process SPS and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from twilio.rest import Client\n",
    "client = Client(\"ACa5636b5b88891fc0789fe740f6d94d8a\", \"2194cba7f961a8b9cdd687040c050514\")\n",
    "FROM = \"+18482510673\"\n",
    "TO = \"+16094015565\"\n",
    "# print '',\n",
    "# sys.stdout.flush()\n",
    "all_sep_spk = {'train': [], 'test': []}\n",
    "all_sep_log = {'train': [], 'test': []}\n",
    "all_label = {'train': [], 'test': []}\n",
    "\n",
    "ds_v = '_v05'\n",
    "\n",
    "h5file = '/Data/DATASETS/CAESAR_TIDIGITS/tidigits_log_sep.h5'\n",
    "\n",
    "h5 = tables.open_file(h5file, 'r')\n",
    "\n",
    "for subset in ['train', 'test']:\n",
    "    \n",
    "    channels = h5.list_nodes(os.path.join(os.sep, 'default', subset))\n",
    "    \n",
    "    for IDX in tqdm(range(len(ALL_NAMES[subset]))):\n",
    "        if IDX in VALID_IDX[subset]:\n",
    "            T, C, w1, w2, fs, _, pos = sps(IDX, subset, sigma=3, adj=1, prior='')\n",
    "\n",
    "            _, utt1, utt2 = re.compile('[A-Z]+').split(ALL_NAMES[subset][IDX])\n",
    "            all_label[subset].append(utt1)\n",
    "            all_label[subset].append(utt2)\n",
    "            if IDX % 500 == 0:\n",
    "                client.messages.create(to=TO, from_=FROM, body=\"SPS: Doing {} of subset {}\".format(IDX, subset))\n",
    "\n",
    "            for i, idx in enumerate([IDX * 2, IDX * 2 + 1]):\n",
    "\n",
    "                K = spike_features(T[i], C[i], limit=True)[2:]\n",
    "                Y = np.reshape(channels[0].features[idx], channels[0].feature_shape[idx])[:, :41].T\n",
    "\n",
    "                env_log = np.sum(Y, 0)\n",
    "                env_sps = np.sum(K, 0)\n",
    "\n",
    "                k = np.correlate(env_log, env_sps, 'full')\n",
    "                shift = np.argmax(np.abs(k)) - len(env_log)\n",
    "\n",
    "                K = K[:, np.abs(shift):]\n",
    "                K = K[:, :Y.shape[1]]\n",
    "\n",
    "                all_sep_spk[subset].append(K)\n",
    "                all_sep_log[subset].append(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ds_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump({'log': all_sep_log['train'], 'spk': all_sep_spk['train'], 'lbl': all_label['train']}, open('all_sep_spk_train{}.pkl'.format(ds_v), 'w'))\n",
    "pkl.dump({'log': all_sep_log['test'], 'spk': all_sep_spk['test'], 'lbl': all_label['test']}, open('all_sep_spk_test{}.pkl'.format(ds_v), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for faulty\n",
    "fixer = {}\n",
    "subset = 'test'\n",
    "for i, a, b in zip(range(len(all_sep_spk)), all_sep_spk[subset], all_sep_log[subset]):\n",
    "    if a.shape[1] != b.shape[1]:\n",
    "        print \"{} / {}  vs  {}\".format(i, a.shape, b.shape)\n",
    "        fixer[i] = np.concatenate([np.zeros((a.shape[0], 1)), a], axis=1)\n",
    "        print \"\\t new size {}\".format(fixer[i].shape)\n",
    "\n",
    "# for k, v in fixer.iteritems():\n",
    "#     all_sep_spk[int(k)] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump({'log': all_sep_log, 'spk': all_sep_spk, 'lbl': all_label['train']}, open('all_sep_spk_train{}.pkl'.format(ds_v), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'train'\n",
    "IDX = 105\n",
    "f, ax = plt.subplots(2, 2)\n",
    "\n",
    "ax[0][0].imshow(all_sep_spk[subset][IDX * 2], aspect='auto')\n",
    "ax[1][0].imshow(all_sep_log[subset][IDX * 2], aspect='auto')\n",
    "ax[0][1].imshow(all_sep_spk[subset][IDX * 2 + 1], aspect='auto')\n",
    "ax[1][1].imshow(all_sep_log[subset][IDX * 2 + 1], aspect='auto')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixer = []\n",
    "IDX = 388\n",
    "subset = 'train'\n",
    "h5file = '/Data/DATASETS/CAESAR_TIDIGITS/tidigits_log_sep.h5'\n",
    "h5 = tables.open_file(h5file, 'r')\n",
    "log_sep = h5.list_nodes(os.path.join(os.sep, 'default', subset))\n",
    "\n",
    "h5file = '/Data/DATASETS/CAESAR_TIDIGITS/tidigits_log_mix.h5'\n",
    "h5 = tables.open_file(h5file, 'r')\n",
    "log_mix = h5.list_nodes(os.path.join(os.sep, 'default', subset))\n",
    "\n",
    "h5file = '/Data/DATASETS/CAESAR_TIDIGITS/tidigits_spk_mix.h5'\n",
    "h5 = tables.open_file(h5file, 'r')\n",
    "spk_mix = h5.list_nodes(os.path.join(os.sep, 'default', subset))\n",
    "\n",
    "f, ax = plt.subplots(2, 3, figsize=(20,10))\n",
    "T, C, w1, w2, fs, _, pos = sps(IDX, subset, sigma=180)\n",
    "_, utt1, utt2 = re.compile('[A-Z]+').split(ALL_NAMES[subset][IDX])\n",
    "        \n",
    "ax[0][2].imshow(np.reshape(spk_mix[0].features[IDX], spk_mix[0].feature_shape[IDX]).T, aspect='auto')\n",
    "ax[1][2].imshow(np.reshape(log_mix[0].features[IDX], log_mix[0].feature_shape[IDX])[:, :41].T, aspect='auto')\n",
    "\n",
    "for i, utt, idx, p in zip([0, 1], [utt1, utt2], [IDX * 2, IDX * 2 + 1], pos):\n",
    "    K = spike_features(T[i], C[i])[2:]\n",
    "    Y = np.reshape(log_sep[0].features[idx], log_sep[0].feature_shape[idx])[:, :41].T\n",
    "    env_log = np.sum(Y, 0)\n",
    "    env_sps = np.sum(K, 0)\n",
    "    k = np.correlate(env_log, env_sps, 'full')\n",
    "    shift = np.argmax(np.abs(k)) - len(env_sps)\n",
    "    K = K[:, np.abs(shift):]\n",
    "    K = K[:, :Y.shape[1]]\n",
    "    ax[0][i].imshow(K, aspect='auto')\n",
    "    ax[0][i].set_title(utt + \" - \" + str(p * 10))\n",
    "    ax[1][i].imshow(Y, aspect='auto')\n",
    "    fixer.append(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sep_spk = {'train': []}\n",
    "all_sep_log = {'train': []}\n",
    "all_label = {'train': []}\n",
    "\n",
    "h5file = '/Data/DATASETS/CAESAR_TIDIGITS/tidigits_spk_sep.h5'\n",
    "h5 = tables.open_file(h5file, 'r')\n",
    "spk = h5.list_nodes(os.path.join(os.sep, 'default', 'train'))\n",
    "\n",
    "h5file2 = '/Data/DATASETS/CAESAR_TIDIGITS/tidigits_log_sep.h5'\n",
    "h52 = tables.open_file(h5file2, 'r')\n",
    "log = h52.list_nodes(os.path.join(os.sep, 'default', 'train'))\n",
    "\n",
    "for idx in range(len(ALL_NAMES['train'])):\n",
    "    _, utt1, utt2 = re.compile('[A-Z]+').split(ALL_NAMES['train'][idx])\n",
    "    all_label['train'].append(utt1)\n",
    "    all_label['train'].append(utt2)\n",
    "\n",
    "for idx in range(len(log[0].features)):\n",
    "\n",
    "    Y = np.reshape(spk[0].features[idx], spk[0].feature_shape[idx]).T\n",
    "    all_sep_spk['train'].append(Y)\n",
    "    \n",
    "    Y = np.reshape(log[0].features[idx], log[0].feature_shape[idx])[:, :41].T\n",
    "    all_sep_log['train'].append(Y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(all_sep_spk['train'])\n",
    "print len(all_sep_log['train'])\n",
    "print len(all_label['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump({'log': all_sep_log['train'], 'spk': all_sep_spk['train'], 'lbl': all_label['train']}, open('all_sep_spk_train.pkl', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test = pkl.load(open('all_sep_spk_test.pkl', 'r'))\n",
    "\n",
    "d_test['spk'][388 * 2] = fixer[0]\n",
    "d_test['spk'][388 * 2 + 1] = fixer[1]\n",
    "\n",
    "all_sep_spk = {'test': d_test['spk']}\n",
    "all_sep_log = {'test': d_test['log']}\n",
    "all_label = {'test': d_test['lbl']}\n",
    "pkl.dump({'log': all_sep_log['test'], 'spk': all_sep_spk['test'], 'lbl': all_label['test']}, open('all_sep_spk_test.pkl', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# VERBOSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixer = []\n",
    "IDX = 4\n",
    "subset = 'train'\n",
    "h5file = '/Data/DATASETS/CAESAR_TIDIGITS/tidigits_log_sep.h5'\n",
    "h5 = tables.open_file(h5file, 'r')\n",
    "log_sep = h5.list_nodes(os.path.join(os.sep, 'default', subset))\n",
    "\n",
    "h5file = '/Data/DATASETS/CAESAR_TIDIGITS/tidigits_log_mix.h5'\n",
    "h5 = tables.open_file(h5file, 'r')\n",
    "log_mix = h5.list_nodes(os.path.join(os.sep, 'default', subset))\n",
    "\n",
    "h5file = '/Data/DATASETS/CAESAR_TIDIGITS/tidigits_spk_mix.h5'\n",
    "h5 = tables.open_file(h5file, 'r')\n",
    "spk_mix = h5.list_nodes(os.path.join(os.sep, 'default', subset))\n",
    "\n",
    "f, ax = plt.subplots(3, 3, figsize=(20,10))\n",
    "T, C, w1, w2, fs, amax, pos = sps(IDX, subset, sigma=3, adj=-1, prior='', verbose=True)\n",
    "_, utt1, utt2 = re.compile('[A-Z]+').split(ALL_NAMES[subset][IDX])\n",
    "        \n",
    "ax[0][2].imshow(np.reshape(spk_mix[0].features[IDX], spk_mix[0].feature_shape[IDX]).T, aspect='auto', cmap=plt.get_cmap('inferno'))\n",
    "ax[1][2].imshow(np.reshape(log_mix[0].features[IDX], log_mix[0].feature_shape[IDX])[:, :41].T, aspect='auto', cmap=plt.get_cmap('inferno'))\n",
    "\n",
    "for i, utt, idx, p in zip([0, 1], [utt1, utt2], [IDX * 2, IDX * 2 + 1], pos):\n",
    "    K = spike_features(T[i], C[i], limit=False)[2:]\n",
    "    Y = np.reshape(log_sep[0].features[idx], log_sep[0].feature_shape[idx])[:, :41].T\n",
    "    env_log = np.sum(Y, 0)\n",
    "    env_sps = np.sum(K, 0)\n",
    "    k = np.correlate(env_log, env_sps, 'full')\n",
    "    shift = np.argmax(np.abs(k)) - len(env_log)\n",
    "#     plt.figure()\n",
    "#     plt.plot(env_log)\n",
    "#     plt.plot(env_sps)\n",
    "    print shift\n",
    "    K = K[:, np.abs(shift):]\n",
    "    K = K[:, :Y.shape[1]]\n",
    "    ax[2][i].plot(T[i], C[i], 'o')\n",
    "    ax[0][i].imshow(K[::-1], aspect='auto', cmap=plt.get_cmap('inferno'))\n",
    "    ax[0][i].set_title(utt + \" - \" + str(p * 10))\n",
    "    ax[1][i].imshow(Y, aspect='auto', cmap=plt.get_cmap('inferno'))\n",
    "    fixer.append(K)\n",
    "\n",
    "a = ax[2][2].hist(amax, bins=range(19), normed=True, label='amax', alpha=0.5)\n",
    "ax[2][2].plot([pos[0] + 0.5, pos[0] + 0.5], [0, a[0][pos[0]]], label='pos0', linewidth=5)\n",
    "ax[2][2].plot([pos[1] + 0.5, pos[1] + 0.5], [0, a[0][pos[1]]], label='pos1', linewidth=5)\n",
    "_ = ax[2][2].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = spike_features(T[0], C[0], limit=True)[2:]\n",
    "Y = np.reshape(log_sep[0].features[idx], log_sep[0].feature_shape[idx])[:, :41].T\n",
    "env_log = np.sum(Y, 0)\n",
    "env_sps = np.sum(K, 0)\n",
    "k = np.correlate(env_log, env_sps, 'full')\n",
    "shift = np.argmax(np.abs(k)) - len(env_log)\n",
    "print shift\n",
    "plt.figure()\n",
    "plt.plot(env_log)\n",
    "plt.figure()\n",
    "plt.plot(env_sps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wind_sp2(ts, ch, w=0.001, limit=False, noise=False):\n",
    "    ts_int = (ts // w).astype('int32')\n",
    "    if len(ts_int) < 1:\n",
    "        ts_int = np.array([0,0])\n",
    "\n",
    "    A = np.zeros((np.max(ts_int) + 3000, 64))\n",
    "    if noise:\n",
    "        A += np.abs(np.random.randn(A.shape[0], A.shape[1]))\n",
    "\n",
    "    for _t, _c in zip(ts_int, ch):\n",
    "        A[_t, _c] += 1\n",
    "        \n",
    "    if limit:\n",
    "        A = np.minimum(A, np.ones_like(A))\n",
    "    # returning [ch, T]\n",
    "    return A.T\n",
    "\n",
    "f, ax = plt.subplots(2,1, figsize=(10,10))\n",
    "ax[0].plot(T[0], C[0], 'o')\n",
    "A = spike_features(T[1], C[1])\n",
    "# A = wind_sp(np.array(T[1]), np.array(C[1]).astype('int32'), limit=True, noise=False)\n",
    "# A = exp_feat(A, win=0.05, l=30, tpe='lap')\n",
    "# A = np.log10(A + 1e-9)\n",
    "# A = simple_low_pass(A.T).T\n",
    "print A.shape\n",
    "ax[1].imshow(A, aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPK ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_streams(streams, max_len, bias=0, fill=-10.):\n",
    "    returned_streams = []\n",
    "    for stream in streams:\n",
    "        num_items = len(stream)\n",
    "        # First dim is batch size, second is length (going to be overwritten to max)\n",
    "        size_tuple = stream[0].shape[1:]\n",
    "        data_tensor = np.ones((num_items, max_len) + size_tuple, dtype='float32') * fill\n",
    "        mask_tensor = np.zeros((num_items, max_len), dtype='float32')\n",
    "        for idx, item in enumerate(stream):\n",
    "            start_offset = 0\n",
    "            if len(item) <= max_len:\n",
    "                data_tensor[idx, start_offset:start_offset + len(item)] = item\n",
    "                mask_tensor[idx, start_offset:start_offset + len(item)] = 1.\n",
    "            else:\n",
    "                if len(item) < bias + max_len:\n",
    "                    data_tensor[idx, start_offset:start_offset + max_len] = item[-max_len:]  # TODO not general\n",
    "                    mask_tensor[idx, start_offset:start_offset + max_len] = 1.\n",
    "                else:\n",
    "                    data_tensor[idx, start_offset:start_offset + max_len] = item[\n",
    "                                                                            bias:bias + max_len]  # TODO not general\n",
    "                    mask_tensor[idx, start_offset:start_offset + max_len] = 1.\n",
    "        returned_streams.append(data_tensor)\n",
    "        returned_streams.append(mask_tensor)\n",
    "    return returned_streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = {'train': {}, 'test': {}}\n",
    "\n",
    "for subset, s in zip(['train', 'test'], [all_sep_spk, all_sep_spk_test]):\n",
    "    keys = ALL_LBL[subset]\n",
    "    for k, v in zip(keys, s):\n",
    "        if k not in DATASET[subset]:\n",
    "            DATASET[subset][k] = []\n",
    "        DATASET[subset][k].append(v.T)\n",
    "\n",
    "for subset in ['train', 'test']:\n",
    "    for k, v in DATASET[subset].iteritems():\n",
    "        DATASET[subset][k] = pad_streams([np.array(v).T], 50)[0]\n",
    "# check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print DATASET['train'][4].shape\n",
    "\n",
    "f, ax = plt.subplots(4, 4)\n",
    "ax[0][0].imshow(DATASET['train'][4][0], aspect='auto')\n",
    "ax[1][0].imshow(DATASET['train'][5][0], aspect='auto')\n",
    "ax[0][1].imshow(DATASET['train'][6][0], aspect='auto')\n",
    "ax[1][1].imshow(DATASET['train'][7][0], aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D, Masking\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "import numpy.random as rng\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "def W_init(shape,name=None):\n",
    "    \"\"\"Initialize weights as in paper\"\"\"\n",
    "    values = rng.normal(loc=0,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)\n",
    "#//TODO: figure out how to initialize layer biases in keras.\n",
    "def b_init(shape,name=None):\n",
    "    \"\"\"Initialize bias as in paper\"\"\"\n",
    "    values=rng.normal(loc=0.5,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)\n",
    "\n",
    "input_shape = (None, 62)\n",
    "left_input = Input(input_shape)\n",
    "right_input = Input(input_shape)\n",
    "#build convnet to use in each siamese 'leg'\n",
    "leg = Sequential()\n",
    "leg.add(Masking(mask_value=-10., input_shape=input_shape))\n",
    "leg.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
    "leg.add(Bidirectional(LSTM(100)))\n",
    "# leg.add(Dense(4096,activation=\"sigmoid\",kernel_regularizer=l2(1e-6),kernel_initializer=W_init,bias_initializer=b_init))\n",
    "leg.add(Dense(512,activation=\"sigmoid\"))\n",
    "\n",
    "#encode each of the two inputs into a vector with the convnet\n",
    "encoded_l = leg(left_input)\n",
    "encoded_r = leg(right_input)\n",
    "#merge two encoded inputs with the l1 distance between them\n",
    "both = Lambda(lambda x: K.abs(x[0]-x[1]))([encoded_l,encoded_r])\n",
    "prediction = Dense(1, activation='sigmoid', bias_initializer=b_init)(both)\n",
    "siamese_net = Model(inputs=[left_input,right_input], outputs=prediction)\n",
    "#optimizer = SGD(0.0004,momentum=0.6,nesterov=True,decay=0.0003)\n",
    "\n",
    "optimizer = Adam()\n",
    "#//TODO: get layerwise learning rates and momentum annealing scheme described in paperworking\n",
    "siamese_net.compile(loss=\"binary_crossentropy\",optimizer=optimizer,metrics=['acc'])\n",
    "\n",
    "siamese_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siamese_Loader:\n",
    "    \"\"\"For loading batches and testing tasks to a siamese net\"\"\"\n",
    "    def __init__(self, x):\n",
    "        self.x = x \n",
    "        self.n_classes = len(x['train'].keys())\n",
    "        self.n_val = len(x['test'].keys())\n",
    "\n",
    "    def get_batch(self, n):\n",
    "        \"\"\"Create batch of n pairs, half same class, half different class\"\"\"\n",
    "        categories = rng.choice(self.n_classes, size=(n,), replace=False)\n",
    "        pairs = [np.zeros((n, 50, 62)) for i in range(2)]\n",
    "        targets = np.zeros((n,))\n",
    "        targets[n//2:] = 1\n",
    "        for i in range(n):\n",
    "            category = categories[i]\n",
    "            idx_1 = rng.randint(0, len(self.x['train'][category]))\n",
    "            pairs[0][i,:,:] = self.x['train'][category][idx_1]\n",
    "            \n",
    "            #pick images of same class for 1st half, different for 2nd\n",
    "            category_2 = category if i >= n//2 else (category + rng.randint(1,self.n_classes)) % self.n_classes\n",
    "            idx_2 = rng.randint(0, len(self.x['train'][category_2]))\n",
    "            pairs[1][i,:,:] = self.x['train'][category_2][idx_2]\n",
    "        return pairs, targets\n",
    "\n",
    "    def make_oneshot_task(self, N):\n",
    "        \"\"\"Create pairs of test image, support set for testing N way one-shot learning. \"\"\"\n",
    "        categories = rng.choice(self.n_val, size=(N,), replace=False)\n",
    "        indices = rng.randint(0, self.n_ex_val, size=(N,))\n",
    "        true_category = categories[0]\n",
    "        ex1, ex2 = rng.choice(self.n_examples, replace=False, size=(2,))\n",
    "        test_image = np.asarray([self.Xval[true_category,ex1,:,:]]*N).reshape(N,self.w,self.h,1)\n",
    "        support_set = self.Xval[categories,indices,:,:]\n",
    "        support_set[0,:,:] = self.Xval[true_category,ex2]\n",
    "        support_set = support_set.reshape(N,self.w,self.h,1)\n",
    "        pairs = [test_image,support_set]\n",
    "        targets = np.zeros((N,))\n",
    "        targets[0] = 1\n",
    "        return pairs, targets\n",
    "\n",
    "    def test_oneshot(self,model,N,k,verbose=0):\n",
    "        \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\"\"\"\n",
    "        pass\n",
    "        n_correct = 0\n",
    "        if verbose:\n",
    "            print(\"Evaluating model on {} unique {} way one-shot learning tasks ...\".format(k,N))\n",
    "        for i in range(k):\n",
    "            inputs, targets = self.make_oneshot_task(N)\n",
    "            probs = model.predict(inputs)\n",
    "            if np.argmax(probs) == 0:\n",
    "                n_correct+=1\n",
    "        percent_correct = (100.0*n_correct / k)\n",
    "        if verbose:\n",
    "            print(\"Got an average of {}% {} way one-shot learning accuracy\".format(percent_correct,N))\n",
    "        return percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_every = 7000\n",
    "loss_every=10\n",
    "batch_size = 100\n",
    "N_way = 20\n",
    "n_val = 550\n",
    "# siamese_net.load_weights(\"PATH\")\n",
    "loader = Siamese_Loader(DATASET)\n",
    "best = 76.0\n",
    "for i in range(900000):\n",
    "    (inputs,targets) = loader.get_batch(batch_size)\n",
    "    loss = siamese_net.train_on_batch(inputs, targets)\n",
    "#     if i % evaluate_every == 0:\n",
    "#         val_acc = loader.test_oneshot(siamese_net,N_way,n_val,verbose=True)\n",
    "#         if val_acc >= best:\n",
    "#             print(\"saving\")\n",
    "#             siamese_net.save('PATH')\n",
    "#             best=val_acc\n",
    "\n",
    "    if i % loss_every == 0:\n",
    "        print(\"iteration {}, training loss: {:.2f}, training acc: {:.2f}\".format(i,loss[0],loss[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
